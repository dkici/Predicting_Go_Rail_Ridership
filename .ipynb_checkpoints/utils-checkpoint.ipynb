{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b004707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd836b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Import Libraries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import strptime\n",
    "import numpy as np\n",
    "import missingno\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from scipy.stats import skew\n",
    "from calendar import day_abbr, month_abbr, mdays\n",
    "import holidays\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66edf048",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "from matplotlib import pyplot\n",
    "from prophet import Prophet\n",
    "import statsmodels\n",
    "import datetime as dt\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e585e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import prophet\n",
    "from prophet.diagnostics import performance_metrics\n",
    "from prophet.diagnostics import cross_validation\n",
    "from prophet.plot import plot_cross_validation_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f2a876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pandas.tseries.offsets import MonthBegin\n",
    "from pandas.tseries.offsets import MonthEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1005dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a better visualization, I will use this plotting parameters\n",
    "\n",
    "# Get current size\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    " \n",
    "# Set figure width to 12 and height to 9\n",
    "fig_size[0] = 30\n",
    "fig_size[1] = 5\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab13b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_time_series(input_path, ts_path):\n",
    "    # ## Load Data\n",
    "\n",
    "    data =  pd.read_excel(input_path)\n",
    "    print(\"data is loaded!\")\n",
    "    # Stripping out spaces from ends of names, and replacing internal spaces or different characters with \"_\"\n",
    "    data.columns = [col.strip().replace(' ', '_').lower() for col in data.columns]\n",
    "    data.columns = [col.strip().replace('-', '_').lower() for col in data.columns]\n",
    "    data.columns = [col.strip().replace('&', '_').lower() for col in data.columns]\n",
    "    data.columns = [col.strip().replace('/', '_').lower() for col in data.columns]\n",
    "\n",
    "    data= data.replace('[\\$,/]', '', regex=True)\n",
    "\n",
    "    # ## Data Exploration\n",
    "\n",
    "    # Any of the features has null data\n",
    "\n",
    "    # -- There are 5 numeric features (all int): Year, Day, Week Number, Ridership and Total Train Trips    \n",
    "    # -- Year, Month, and Day can be combined to create Date feature\n",
    "    # -- Month, Rail Corridor, Weekend&Holidays/weekday, Station, and Time Period are categorical \n",
    "\n",
    "    categorical_features = data[['month', 'rail_corridor', 'weekend_holidays_weekday', 'station', 'time_period']]\n",
    "\n",
    "    #Summary of the numerical features\n",
    "    numeric_features = data[['year','day','week_number','ridership','total_train_trips']]\n",
    "\n",
    "    # ## Feature Engineering\n",
    "\n",
    "    # Since this is a time series analysis, I create a Date column\n",
    "\n",
    "    data['Month'] = [strptime(str(i), '%B').tm_mon for i in data['month']]\n",
    "    data[\"date\"] = pd.to_datetime(data[['year', 'Month', 'day']])\n",
    "\n",
    "    # data[\"ridership_by_trips\"] = data[\"ridership\"]/data[\"total_train_trips\"]\n",
    "    # data.head()\n",
    "\n",
    "    # Since this is time series data, I will create time series for each different staion in each rail corridors.\n",
    "\n",
    "    data[\"weekend_holidays_weekday\"].unique()\n",
    "\n",
    "    data = data.drop(columns = 'weekend_holidays_weekday')\n",
    "\n",
    "    df1 = pd.pivot_table(data, values=['ridership'], index=['date'],\n",
    "                           columns=['rail_corridor'], aggfunc=np.sum) #, fill_value=0\n",
    "    df1.columns = [' '.join(col).strip() for col in df1.columns.values]\n",
    "    df1.index.freq = 'd'\n",
    "    missingno.bar(df1)\n",
    "\n",
    "    # These time series give us the total number of ridership (7 time series) and total_train_trips (7 time series) based for each rail_corridor. The lenght of each time series is 1461 (4 years data between 2019-2022) \n",
    "    # We can use the time components (year, month, day, and week numbers) as covariants as well as weekend/weekday and holiday or not.\n",
    "    # But we loose the peak time numbers of the weekdays in this level. The highest scacity is about 30%.\n",
    "\n",
    "    df2 = pd.pivot_table(data, values=['ridership'], index=['date'],\n",
    "                           columns=['rail_corridor','station'], aggfunc=np.sum) #, fill_value=0\n",
    "    df2.columns = [' '.join(col).strip() for col in df2.columns.values]\n",
    "    df2.index.freq = 'd'\n",
    "    missingno.bar(df2)\n",
    "\n",
    "    # These time series gives us total number of ridership (58 time series) and total_train_trips (58 time series) for each station in each rail corridor. Similarly, we can use the time components (year, month, day, and week numbers) as covariants as well as weekend/weekday and holiday or not. But we still loose the peak time numbers of the weekdays in this level.\n",
    "\n",
    "    # Let's check the scarcity of the time series\n",
    "\n",
    "    df3 = pd.pivot_table(data, values=['ridership'], index=['date'],\n",
    "                           columns=['rail_corridor','station', 'time_period'], aggfunc=np.sum) #, fill_value=0\n",
    "    df3.columns = [' '.join(col).strip() for col in df3.columns.values]\n",
    "    df3.index.freq = 'd'\n",
    "    missingno.bar(df3) \n",
    "\n",
    "    # These third level time series gives us total number of ridership (179 time series) and total_train_trips (179 time series)\n",
    "    # for different peak days/times for each station in each rail corridor. \n",
    "    # This allows us to make our predictions for specific time period by suing not onlu the time components (year, month, day, and week numbers) as covariants as well as weekend/weekday and holiday or not.\n",
    "    # but also specific days and times of the days.\n",
    "    # \n",
    "    # For example, the first level time series allow us to make prediction only for a specific region like Barrie.\n",
    "    # To make predictions for specific stations in Barries such as AURORA GO station, we need the second level time series.\n",
    "    # With the third level time series we can forecast the number of ridership and/or total_train_trips in Barrie, AURORA GO station \n",
    "    # in the Evening time, in the mid-day, or at PM peak. \n",
    "    # \n",
    "    # On the other hand, as it is shown in the bar graphs above, the scarcity of these time series for the second and third level data is high. Then, most of these time series are not proper to be used to make predictions for that that specific times of the days. \n",
    "    # \n",
    "    # I will select a few time series from each level for forecasting.\n",
    "\n",
    "    data.to_csv(f\"{ts_path}/Metrolinx_prepocessed_data.csv\")\n",
    "\n",
    "    # ### Create Time Series at different Levels\n",
    "    # Rail Corridor\n",
    "    for rc in data.rail_corridor.unique(): \n",
    "        df_rc = data[(data[\"rail_corridor\"] == rc)]\n",
    "        df_rc.rail_corridor.unique()\n",
    "\n",
    "        df_rc = pd.pivot_table(df_rc, values=['ridership', 'total_train_trips'], index=['date'],\n",
    "                                               columns=['rail_corridor'], aggfunc=np.sum, fill_value=0)\n",
    "\n",
    "        df_rc.columns = [' '.join(col).strip() for col in df_rc.columns.values]\n",
    "        # df_rc.index.freq = 'd'\n",
    "        if len(df_rc) > 1000: #(4*365)*0.75:\n",
    "            df_rc.to_csv(f\"{ts_path}/Metrolinx_{rc}_Data.csv\")\n",
    "\n",
    "        # Rail Corridor and Station\n",
    "        for st in data.station.unique():\n",
    "            df_rc_st = data[(data[\"rail_corridor\"] == rc) & (data[\"station\"] == st)]\n",
    "            df_rc_st = pd.pivot_table(df_rc_st, values=['ridership', 'total_train_trips'], index=['date'],\n",
    "                           columns=['rail_corridor', 'station'], aggfunc=np.sum, fill_value=0)\n",
    "            df_rc_st.columns = [' '.join(col).strip() for col in df_rc_st.columns.values]\n",
    "    #         df_rc_st.index.freq = 'd'\n",
    "            if len(df_rc_st) > 1000: #(4*365)*0.75:\n",
    "                df_rc_st.to_csv(f\"{ts_path}/Metrolinx_{rc}_{st}_Data.csv\")\n",
    "\n",
    "            # Rail Corridor, Station, and time_period\n",
    "            for tp in data.time_period.unique():\n",
    "                df_rc_st_tp = data[(data[\"rail_corridor\"] == rc) & (data[\"station\"] == st) & (data[\"time_period\"] == tp)]\n",
    "                df_rc_st_tp = pd.pivot_table(df_rc_st_tp, values=['ridership', 'total_train_trips'], index=['date'],\n",
    "                               columns=['rail_corridor', 'station', 'time_period'], aggfunc=np.sum, fill_value=0)\n",
    "                df_rc_st_tp.columns = [' '.join(col).strip() for col in df_rc_st_tp.columns.values]\n",
    "    #             df_rc_st_tp.index.freq = 'd'\n",
    "                if len(df_rc_st_tp) > 1000: #(4*365)*0.75:\n",
    "                    df_rc_st_tp.to_csv(f\"{ts_path}/Metrolinx_{rc}_{st}_{tp}_Data.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75fdc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_timeseries(ts_data_path):\n",
    "    data_all = pd.read_csv(ts_data_path)\n",
    "    print('time series loaded!')\n",
    "    column_names = ['date', 'ridership', 'trips']\n",
    "    data_all.columns = column_names\n",
    "#     print(data_all.head())\n",
    "#\n",
    "#     print(data_all.info())\n",
    "#     print(len(data_all)) \n",
    "    #Imputing missing in the timestamps\n",
    "   \n",
    "    data_all['date'] = pd.to_datetime(data_all['date']) + dt.timedelta(days=1)\n",
    "    data_all = data_all.set_index('date')\n",
    "    data_all = data_all.resample('1D').mean()\n",
    "#     print(data_all.index)\n",
    "\n",
    "    data_all.isnull().sum()\n",
    "    data_all = data_all.fillna('0')\n",
    "#     print(data_all.isnull().sum())\n",
    "\n",
    "#     print(data_all.info())\n",
    "    data_all = data_all.astype(int)\n",
    "#     print(data_all.info())\n",
    "    print(\"Data All: \\n \", data_all.head())\n",
    "    \n",
    "    data = data_all.filter([\"date\",\"ridership\"])\n",
    "    print(\"Data: \\n \", data.head())\n",
    "    \n",
    "    data_regressors = data_all.filter([\"date\", \"trips\"])\n",
    "    print(\"Data Regressors: \\n \", data_regressors.head())\n",
    "    \n",
    "    return data, data_all, data_regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93cf5a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_eda(data):\n",
    "    data.reset_index().plot(x='date', y='ridership', kind='line', grid=1)\n",
    "    plt.pyplot.show()\n",
    "\n",
    "#     adf, pval, usedlag, nobs, crit_vals, icbest =  adfuller(data.ridership.values)\n",
    "#     print('ADF test statistic:', adf)\n",
    "#     print('ADF p-values:', pval)\n",
    "#     print('ADF number of lags used:', usedlag)\n",
    "#     print('ADF number of observations:', nobs)\n",
    "#     print('ADF critical values:', crit_vals)\n",
    "#     print('ADF best information criterion:', icbest)\n",
    "\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data_decompose_add = seasonal_decompose(data, model='additive')\n",
    "    data_decompose_add.plot().show()\n",
    "\n",
    "    data_decompose_add_resid = data_decompose_add.resid.sum()\n",
    "    print(\"additive residual\" ,data_decompose_add_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28403764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_adfuller(ts):\n",
    "    # Dickey-Fuller test\n",
    "    result = adfuller(ts, autolag='AIC')\n",
    "    print('Test statistic: ' , result[0])\n",
    "    print('p-value: '  ,result[1])\n",
    "    print('Critical Values:' ,result[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acf_pacf(ts):    \n",
    "    # ACF and PACF \n",
    "    from statsmodels.tsa.stattools import acf, pacf\n",
    "    lag_acf = acf(ts_diff, nlags=20)\n",
    "    lag_pacf = pacf(ts_diff, nlags=20, method='ols')\n",
    "    # ACF\n",
    "    plt.figure(figsize=(22,10))\n",
    "\n",
    "    plt.subplot(121) \n",
    "    plt.plot(lag_acf)\n",
    "    plt.axhline(y=0,linestyle='--',color='gray')\n",
    "    plt.axhline(y=-1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
    "    plt.axhline(y=1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
    "    plt.title('Autocorrelation Function')\n",
    "\n",
    "    # PACF\n",
    "    plt.subplot(122)\n",
    "    plt.plot(lag_pacf)\n",
    "    plt.axhline(y=0,linestyle='--',color='gray')\n",
    "    plt.axhline(y=-1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
    "    plt.axhline(y=1.96/np.sqrt(len(ts_diff)),linestyle='--',color='gray')\n",
    "    plt.title('Partial Autocorrelation Function')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e17f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_block(verif, start_date, end_date, ax=None): \n",
    "    df = verif.loc[start_date:end_date,:]\n",
    "    df.loc[:,'yhat'].plot(lw=2, ax=ax, color='r', ls='-', label='forecasts')\n",
    "    ax.fill_between(df.index, df.loc[:,'yhat_lower'], df.loc[:,'yhat_upper'], color='coral', alpha=0.3)\n",
    "    df.loc[:,'y'].plot(lw=2, ax=ax, color='steelblue', ls='-', label='observations')\n",
    "    ax.grid(ls=':')\n",
    "    ax.legend(fontsize=15)\n",
    "    [l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "    [l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\n",
    "    ax.set_ylabel('piece number', fontsize=15)\n",
    "    ax.set_xlabel('', fontsize=15)\n",
    "    ax.set_title(f'{start_date} to {end_date}', fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6dfa36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor_index(m, name):\n",
    "    \"\"\"Given the name of a regressor, return its (column) index in the `beta` matrix.\n",
    "    Parameters\n",
    "    ----------\n",
    "    m: Prophet model object, after fitting.\n",
    "    name: Name of the regressor, as passed into the `add_regressor` function.\n",
    "    Returns\n",
    "    -------\n",
    "    The column index of the regressor in the `beta` matrix.\n",
    "    \"\"\"\n",
    "    return np.extract(\n",
    "        m.train_component_cols[name] == 1, m.train_component_cols.index\n",
    "    )[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcff4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor_coefficients(m):\n",
    "    \"\"\"Summarise the coefficients of the extra regressors used in the model.\n",
    "    For additive regressors, the coefficient represents the incremental impact\n",
    "    on `y` of a unit increase in the regressor. For multiplicative regressors,\n",
    "    the incremental impact is equal to `trend(t)` multiplied by the coefficient.\n",
    "    Coefficients are measured on the original scale of the training data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    m: Prophet model object, after fitting.\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame containing:\n",
    "    - `regressor`: Name of the regressor\n",
    "    - `regressor_mode`: Whether the regressor has an additive or multiplicative\n",
    "        effect on `y`.\n",
    "    - `center`: The mean of the regressor if it was standardized. Otherwise 0.\n",
    "    - `coef_lower`: Lower bound for the coefficient, estimated from the MCMC samples.\n",
    "        Only different to `coef` if `mcmc_samples > 0`.\n",
    "    - `coef`: Expected value of the coefficient.\n",
    "    - `coef_upper`: Upper bound for the coefficient, estimated from MCMC samples.\n",
    "        Only to different to `coef` if `mcmc_samples > 0`.\n",
    "    \"\"\"\n",
    "    assert len(m.extra_regressors) > 0, 'No extra regressors found.'\n",
    "    coefs = []\n",
    "    for regressor, params in m.extra_regressors.items():\n",
    "        beta = m.params['beta'][:, regressor_index(m, regressor)]\n",
    "        if params['mode'] == 'additive':\n",
    "            coef = beta * m.y_scale / params['std']\n",
    "        else:\n",
    "            coef = beta / params['std']\n",
    "        percentiles = [\n",
    "            (1 - m.interval_width) / 2,\n",
    "            1 - (1 - m.interval_width) / 2,\n",
    "        ]\n",
    "        coef_bounds = np.quantile(coef, q=percentiles)\n",
    "        record = {\n",
    "            'regressor': regressor,\n",
    "            'regressor_mode': params['mode'],\n",
    "            'center': params['mu'],\n",
    "            'coef_lower': coef_bounds[0],\n",
    "            'coef': np.mean(coef),\n",
    "            'coef_upper': coef_bounds[1],\n",
    "        }\n",
    "        coefs.append(record)\n",
    "\n",
    "    return pd.DataFrame(coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3154fbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a553cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a753d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
