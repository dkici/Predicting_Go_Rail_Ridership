{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import calendar\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "def load_data(path):\n",
    "    data_all = pd.read_csv({path)\n",
    "    column_names = ['date', 'ridership', 'trips']\n",
    "    data_all.columns = column_names\n",
    "    print(data_all.head())\n",
    "    \n",
    "    \n",
    "# def preprocessing_pieces_actuals(path):\n",
    "    \n",
    "#     #Load and read piece movement data\n",
    "#     df_pieces = pd.read_excel(f'{path}/data_pieces_Jan2019_Jan2023.xlsx')\n",
    "#     df_pieces.columns = df_pieces.iloc[0,:]\n",
    "#     df_pieces = df_pieces.iloc[1:-2,:]\n",
    "#     df_pieces = df_pieces[['WeekStartDate', \n",
    "#                            'CPU pieces',\n",
    "#                            'In-lane repair pieces',\n",
    "#                            'In-shop repair pieces',\n",
    "#                            'Outbound Load pieces',\n",
    "#                            'Outside repair pieces',\n",
    "#                            'Picking pieces',\n",
    "#                            'Prep/Assembly pieces',\n",
    "#                            'Put-a-way pieces',\n",
    "#                            'Receiving Unload pieces',\n",
    "#                            'SNL-rewrap pieces']]\n",
    "\n",
    "#     df_pieces = df_pieces.rename(columns={'WeekStartDate':'datetime'})\n",
    "#     df_pieces['datetime'] = pd.to_datetime(df_pieces['datetime'] )\n",
    "#     df_pieces.set_index('datetime', inplace=True)\n",
    "\n",
    "#     # df_pieces = df_pieces.astype('float')\n",
    "#     # df_pieces.info()\n",
    "\n",
    "#     df_pieces.columns = [col.strip().replace(' ', '_').lower() for col in df_pieces.columns]\n",
    "#     df_pieces.columns = [col.strip().replace('-', '_').lower() for col in df_pieces.columns]\n",
    "#     df_pieces.columns = [col.strip().replace('/', '_').lower() for col in df_pieces.columns]\n",
    "\n",
    "#     df_pieces[df_pieces.columns] = df_pieces[df_pieces.columns].replace('[\\$,()]', '', regex=True).astype(float)\n",
    "#     df_pieces.info()\n",
    "\n",
    "#     df_pieces['total_of_6_pieces'] = df_pieces.sum(axis=1)\n",
    "#     df_pieces  = df_pieces.fillna(0)\n",
    "#     df_pieces.isnull().sum()\n",
    "#     print(df_pieces)\n",
    "    \n",
    "#     #Load and read piece actual_sales data\n",
    "#     df_actuals = pd.read_excel(f'{path}/actual_sales_Jan2019_Jan2023.xlsx')\n",
    "#     df_actuals = df_actuals.iloc[1:-3,:]\n",
    "#     df_actuals = df_actuals.rename(columns = {'Category':'datetime'})\n",
    "#     df_actuals = df_actuals.rename(columns = {'Unnamed: 1':'unknown_cat'})\n",
    "#     df_actuals.set_index('datetime', inplace = True)\n",
    "#     df_actuals = df_actuals.drop(columns={'unknown_cat'}, axis=1)\n",
    "\n",
    "#     df_actuals.columns = [col.strip().replace(' ', '_').lower() for col in df_actuals.columns]\n",
    "#     df_actuals.columns = [col.strip().replace('-', '_').lower() for col in df_actuals.columns]\n",
    "#     df_actuals.columns = [col.strip().replace('/', '_').lower() for col in df_actuals.columns]\n",
    "#     # df_actuals = df_actuals.rename(columns= lambda col: col+'_actuals')\n",
    "#     df_actuals[df_actuals.columns] = df_actuals[df_actuals.columns].replace('[\\$,()]', '', regex=True).astype(float)\n",
    "\n",
    "#     df_actuals.info()\n",
    "#     df_actuals.head()\n",
    "#     df_actuals.isnull().sum()\n",
    "#     df_actuals = df_actuals.fillna(0)\n",
    "#     df_actuals.isnull().sum()\n",
    "    \n",
    "#     #Merge Piece and T Sales Data As Training Data\n",
    "#     df_actuals.reset_index(inplace = True)\n",
    "#     df_pieces.reset_index(inplace = True)\n",
    "#     df_actuals = df_actuals.merge(df_pieces, on = 'datetime')\n",
    "#     df_actuals.columns\n",
    "    \n",
    "#     # Now converting \"Date\"  to date time\n",
    "#     df_actuals[\"datetime\"]=pd.to_datetime(df_actuals['datetime'])\n",
    "#     df_actuals[\"Week\"]=df_actuals['datetime'].dt.week\n",
    "#     df_actuals[\"Month\"]=df_actuals['datetime'].dt.month\n",
    "#     df_actuals[\"Year\"]=df_actuals['datetime'].dt.year\n",
    "#     # Changing the Months value from numbers to real values like Jan, Feb to Dec\n",
    "#     df_actuals['Month'] = df_actuals['Month'].apply(lambda x: calendar.month_abbr[x])\n",
    "#     df_actuals = df_actuals.iloc[1:,:]\n",
    "    \n",
    "#     #between Jan 2021- Dec 2023\n",
    "#     print(\"Actual Sales and Pieces Data\")\n",
    "#     df_actuals.to_csv(f'{path}\\\\romeville_cat_sales_pieces_data_cleaned_processed_TSales_2021_2023.csv')\n",
    "    \n",
    "#     return df_actuals\n",
    "    \n",
    "\n",
    "# def preprocessing_promo(path):\n",
    "#     #Promo Calender Data\n",
    "#     promo_df = pd.read_csv(f\"C:\\\\Users\\\\DKici\\\\Documents\\\\Projects\\\\DC_Staffing\\\\data\\\\promos_by_week.csv\")\n",
    "#     promo_df = promo_df.rename(columns ={\"week_starting\":\"ds\"})\n",
    "#     promo_df[\"IsPromo\"] = promo_df.iloc[:,1:-1].sum(axis = 1)\n",
    "#     promo_df.head()\n",
    "#     promo_df.ds = pd.to_datetime(promo_df.ds)\n",
    "#     promo_df.ds = promo_df.ds + timedelta(days=1)\n",
    "#     return promo_df\n",
    "\n",
    "# def preprocessing_holiday(path):\n",
    "#     holidays = pd.read_csv('C:\\\\Users\\\\DKici\\\\Documents\\\\Projects\\\\DC_Staffing\\\\Prophet\\\\data\\\\holidays.csv')\n",
    "#     holidays.columns= holidays.columns.str.lower()\n",
    "#     # for t_col in ['DS', 'DS_UPPER']:\n",
    "#     #     holidays[t_col] = pd.to_datetime(holidays[t_col])\n",
    "#     # holidays['upper_window'] = (holidays['DS_UPPER'] - holidays['DS']).dt.days\n",
    "#     holidays = holidays.iloc[:,:-1]\n",
    "#     return holidays\n",
    "\n",
    "# def preprocessing_budget(path):\n",
    "    \n",
    "#     #Load and read piece budget_sales data\n",
    "#     df_budget= pd.read_excel(f'{path}/budget_sales_Jan2022_Dec2023.xlsx')\n",
    "#     df_budget = df_budget.iloc[1:-3,:] \n",
    "#     df_budget = df_budget.rename(columns = {'CategoryID':'datetime'})\n",
    "#     df_budget.set_index('datetime', inplace = True)\n",
    "#     df_budget.columns = [col.strip().replace(' ', '_').lower() for col in df_budget.columns]\n",
    "#     df_budget.columns = [col.strip().replace('-', '_').lower() for col in df_budget.columns]\n",
    "#     df_budget.columns = [col.strip().replace('/', '_').lower() for col in df_budget.columns]\n",
    "\n",
    "#     df_budget[df_budget.columns] = df_budget[df_budget.columns].replace('[\\$,()]', '', regex=True).astype(float)\n",
    "#     # df_budget = df_budget.rename(columns= lambda col: col+'_budget')\n",
    "#     df_budget.info()\n",
    "#     df_budget.head()\n",
    "#     df_budget.isnull().sum()\n",
    "#     # save budget data as forecasting data\n",
    "#     print(\"Budget Data\")\n",
    "#     df_budget.to_csv(f'{path}\\\\romeville_cat_data_cleaned_processed_budget_for_forecasting.csv')\n",
    "    \n",
    "#     return df_budget\n",
    "\n",
    "\n",
    "# def median_filter(df, varname = None, window=24, std=3): \n",
    "#     \"\"\"\n",
    "#     A simple median filter, removes (i.e. replace by np.nan) observations that exceed N (default = 3) \n",
    "#     tandard deviation from the median over window of length P (default = 24) centered around \n",
    "#     each observation.\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     df : pandas.DataFrame\n",
    "#         The pandas.DataFrame containing the column to filter.\n",
    "#     varname : string\n",
    "#         Column to filter in the pandas.DataFrame. No default. \n",
    "#     window : integer \n",
    "#         Size of the window around each observation for the calculation \n",
    "#         of the median and std. Default is 24 (time-steps).\n",
    "#     std : integer \n",
    "#         Threshold for the number of std around the median to replace \n",
    "#         by `np.nan`. Default is 3 (greater / less or equal).\n",
    "#     Returns\n",
    "#     -------\n",
    "#     dfc : pandas.Dataframe\n",
    "#         A copy of the pandas.DataFrame `df` with the new, filtered column `varname`\n",
    "#     \"\"\"\n",
    "    \n",
    "#     dfc = df.loc[:,[varname]]\n",
    "    \n",
    "#     dfc['median']= dfc[varname].rolling(window, center=True).median()\n",
    "    \n",
    "#     dfc['std'] = dfc[varname].rolling(window, center=True).std()\n",
    "    \n",
    "#     dfc.loc[dfc.loc[:,varname] >= dfc['median']+std*dfc['std'], varname] = np.nan\n",
    "    \n",
    "#     dfc.loc[dfc.loc[:,varname] <= dfc['median']-std*dfc['std'], varname] = np.nan\n",
    "    \n",
    "#     return dfc.loc[:, varname]\n",
    "\n",
    "# def prepare_data(data, year=2017): \n",
    "#     \"\"\"\n",
    "#     prepare the data for ingestion by fbprophet: \n",
    "#     see: https://facebook.github.io/prophet/docs/quick_start.html\n",
    "    \n",
    "#     1) divide in training and test set, using the `year` parameter (int)\n",
    "    \n",
    "#     2) reset the index and rename the `datetime` column to `ds`\n",
    "    \n",
    "#     returns the training and test dataframes\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data : pandas.DataFrame \n",
    "#         The dataframe to prepare, needs to have a datetime index\n",
    "#     year: integer \n",
    "#         The year separating the training set and the test set (includes the year)\n",
    "#     Returns\n",
    "#     -------\n",
    "#     data_train : pandas.DataFrame\n",
    "#         The training set, formatted for fbprophet.\n",
    "#     data_test :  pandas.Dataframe\n",
    "#         The test set, formatted for fbprophet.\n",
    "#     \"\"\"\n",
    "    \n",
    "    \n",
    "#     data_train = data.loc[:str(year - 1),:]\n",
    "    \n",
    "#     data_test = data.loc[str(year):,:]\n",
    "    \n",
    "#     data_train.reset_index(inplace=True)\n",
    "    \n",
    "#     data_test.reset_index(inplace=True)\n",
    "    \n",
    "#     data_train = data_train.rename({'datetime':'ds'}, axis=1)\n",
    "    \n",
    "#     data_test = data_test.rename({'datetime':'ds'}, axis=1)\n",
    "    \n",
    "#     return data_train, data_test\n",
    "\n",
    "\n",
    "# def add_regressor(data, regressor, varname=None): \n",
    "    \n",
    "#     \"\"\"\n",
    "#     adds a regressor to a `pandas.DataFrame` of target (predictand) values \n",
    "#     for use in fbprophet \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data : pandas.DataFrame \n",
    "#         The pandas.DataFrame in the fbprophet format (see function `prepare_data` in this package)\n",
    "#     regressor : pandas.DataFrame \n",
    "#         A pandas.DataFrame containing the extra-regressor\n",
    "#     varname : string \n",
    "#         The name of the column in the `regressor` DataFrame to add to the `data` DataFrame\n",
    "#     Returns\n",
    "#     -------\n",
    "#     verif : pandas.DataFrame\n",
    "#         The original `data` DataFrame with the column containing the \n",
    "#         extra regressor `varname`\n",
    "#     \"\"\"\n",
    "\n",
    "#     data_with_regressors = data.copy()\n",
    "    \n",
    "#     data_with_regressors.loc[:,varname] = regressor.loc[:,varname]\n",
    "    \n",
    "#     return data_with_regressors\n",
    "\n",
    "# def add_regressor_to_future(future, regressors_df): \n",
    "#     \"\"\"\n",
    "#     adds extra regressors to a `future` DataFrame dataframe created by fbprophet\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     data : pandas.DataFrame\n",
    "#         A `future` DataFrame created by the fbprophet `make_future` method  \n",
    "        \n",
    "#     regressors_df: pandas.DataFrame \n",
    "#         The pandas.DataFrame containing the regressors (with a datetime index)\n",
    "#     Returns\n",
    "#     -------\n",
    "#     futures : pandas.DataFrame\n",
    "#         The `future` DataFrame with the regressors added\n",
    "#     \"\"\"\n",
    "    \n",
    "#     futures = future.copy() \n",
    "    \n",
    "#     futures.index = pd.to_datetime(futures.ds)\n",
    "    \n",
    "#     regressors = pd.concat(regressors_df, axis=1)\n",
    "    \n",
    "#     futures = futures.merge(regressors, left_index=True, right_index=True)\n",
    "    \n",
    "#     futures = futures.reset_index(drop = True)\n",
    "    \n",
    "#     return futures\n",
    "\n",
    "\n",
    "# def make_verif(forecast, data_train, data_test): \n",
    "#     \"\"\"\n",
    "#     Put together the forecast (coming from fbprophet) \n",
    "#     and the overved data, and set the index to be a proper datetime index, \n",
    "#     for plotting\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     forecast : pandas.DataFrame \n",
    "#         The pandas.DataFrame coming from the `forecast` method of a fbprophet \n",
    "#         model. \n",
    "    \n",
    "#     data_train : pandas.DataFrame\n",
    "#         The training set, pandas.DataFrame\n",
    "#     data_test : pandas.DataFrame\n",
    "#         The training set, pandas.DataFrame\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     forecast : \n",
    "#         The forecast DataFrane including the original observed data.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     forecast.index = pd.to_datetime(forecast.ds)\n",
    "    \n",
    "#     data_train.index = pd.to_datetime(data_train.ds)\n",
    "    \n",
    "#     data_test.index = pd.to_datetime(data_test.ds)\n",
    "    \n",
    "#     data = pd.concat([data_train, data_test], axis=0)\n",
    "    \n",
    "#     forecast.loc[:,'y'] = data.loc[:,'y']\n",
    "    \n",
    "#     return forecast\n",
    "\n",
    "# def plot_verif(verif, year=2017):\n",
    "#     \"\"\"\n",
    "#     plots the forecasts and observed data, the `year` argument is used to visualise \n",
    "#     the division between the training and test sets. \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     verif : pandas.DataFrame\n",
    "#         The `verif` DataFrame coming from the `make_verif` function in this package\n",
    "#     year : integer\n",
    "#         The year used to separate the training and test set. Default 2017\n",
    "#     Returns\n",
    "#     -------\n",
    "#     f : matplotlib Figure object\n",
    "#     \"\"\"\n",
    "    \n",
    "#     f, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "#     train = verif.loc[:str(year - 1),:]\n",
    "    \n",
    "#     ax.plot(train.index, train.y, 'ko', markersize=3)\n",
    "    \n",
    "#     ax.plot(train.index, train.yhat, color='steelblue', lw=0.5)\n",
    "    \n",
    "#     ax.fill_between(train.index, train.yhat_lower, train.yhat_upper, color='steelblue', alpha=0.3)\n",
    "    \n",
    "#     test = verif.loc[str(year):,:]\n",
    "    \n",
    "#     ax.plot(test.index, test.y, 'ro', markersize=3)\n",
    "    \n",
    "#     ax.plot(test.index, test.yhat, color='coral', lw=0.5)\n",
    "    \n",
    "#     ax.fill_between(test.index, test.yhat_lower, test.yhat_upper, color='coral', alpha=0.3)\n",
    "    \n",
    "#     ax.axvline(str(year), color='0.8', alpha=0.7)\n",
    "    \n",
    "#     ax.grid(ls=':', lw=0.5)\n",
    "    \n",
    "#     return f\n",
    "\n",
    "# def plot_verif_component(verif, component='rain', year=2017): \n",
    "#     \"\"\"\n",
    "#     plots a specific component of the `verif` DataFrame\n",
    "#    Parameters\n",
    "#     ----------\n",
    "#     verif : pandas.DataFrame\n",
    "#         The `verif` DataFrame coming from the `make_verif` function in this package. \n",
    "#     component : string \n",
    "#         The name of the component (i.e. column name) to plot in the `verif` DataFrame. \n",
    "#     year : integer\n",
    "#         The year used to separate the training and test set. Default 2017\n",
    "#     Returns\n",
    "#     -------\n",
    "#     f : matplotlib Figure object\n",
    "#     \"\"\"\n",
    "    \n",
    "#     f, ax = plt.subplots(figsize=(14, 7))\n",
    "    \n",
    "#     train = verif.loc[:str(year - 1),:]\n",
    "        \n",
    "#     ax.plot(train.index, train.loc[:,component] * 100, color='0.8', lw=1, ls='-')\n",
    "    \n",
    "#     ax.fill_between(train.index, train.loc[:, component+'_lower'] * 100, train.loc[:, component+'_upper'] * 100, color='0.8', alpha=0.3)\n",
    "    \n",
    "#     test = verif.loc[str(year):,:]\n",
    "        \n",
    "#     ax.plot(test.index, test.loc[:,component] * 100, color='k', lw=1, ls='-')\n",
    "    \n",
    "#     ax.fill_between(test.index, test.loc[:, component+'_lower'] * 100, test.loc[:, component+'_upper'] * 100, color='0.8', alpha=0.3)\n",
    "    \n",
    "#     ax.axvline(str(year), color='k', alpha=0.7)\n",
    "    \n",
    "#     ax.grid(ls=':', lw=0.5)\n",
    "    \n",
    "#     return f\n",
    "\n",
    "\n",
    "# def plot_joint_plot(verif, x='yhat', y='y', title=None, fpath = '../figures/paper', fname = None): \n",
    "#     \"\"\"\n",
    "    \n",
    "#     Parameters\n",
    "#     ---------- \n",
    "#     verif : pandas.DataFrame \n",
    "#     x : string \n",
    "#         The variable on the x-axis\n",
    "#         Defaults to `yhat`, i.e. the forecast or estimated values.\n",
    "#     y : string \n",
    "#         The variable on the y-axis\n",
    "#         Defaults to `y`, i.e. the observed values\n",
    "#     title : string \n",
    "#         The title of the figure, default `None`. \n",
    "    \n",
    "#     fpath : string \n",
    "#         The path to save the figures, default to `../figures/paper`\n",
    "#     fname : string\n",
    "#         The filename for the figure to be saved\n",
    "#         ommits the extension, the figure is saved in png, jpeg and pdf\n",
    " \n",
    "#     Returns\n",
    "#     -------\n",
    "#     f : matplotlib Figure object\n",
    "#     \"\"\"\n",
    "\n",
    "#     g = sns.jointplot(x='yhat', y='y', data = verif, kind=\"reg\", color=\"0.4\")\n",
    "    \n",
    "#     g.fig.set_figwidth(8)\n",
    "#     g.fig.set_figheight(8)\n",
    "\n",
    "#     ax = g.fig.axes[1]\n",
    "    \n",
    "#     if title is not None: \n",
    "#         ax.set_title(title, fontsize=16)\n",
    "\n",
    "#     ax = g.fig.axes[0]\n",
    "\n",
    "#     ax.set_xlim([-5, None])\n",
    "#     ax.set_ylim([-5, 3000])\n",
    "\n",
    "#     ax.text(100, 2500, \"R = {:+4.2f}\\nMAE = {:4.1f}\".format(verif.loc[:,['y','yhat']].corr().iloc[0,1], MAE(verif.loc[:,'y'].values, verif.loc[:,'yhat'].values)), fontsize=16)\n",
    "\n",
    "#     ax.set_xlabel(\"model's estimates\", fontsize=15)\n",
    "    \n",
    "#     ax.set_ylabel(\"observations\", fontsize=15)\n",
    "    \n",
    "#     ax.grid(ls=':')\n",
    "\n",
    "#     [l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n",
    "#     [l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()];\n",
    "\n",
    "#     ax.grid(ls=':')\n",
    "    \n",
    "#     if fname is not None: \n",
    "#         for ext in ['png','jpeg','pdf']: \n",
    "#             g.fig.savefig(os.path.join(fpath, \"{}.{}\".format(fname, ext)), dpi=200)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e24824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
